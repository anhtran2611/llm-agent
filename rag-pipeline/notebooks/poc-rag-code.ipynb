{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hieu/Projects/tiny-llm-agent/rag-pipeline/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n",
    "Make sure we have an activated virtual environment running Python 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "Hardware is set to:  mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.backends.mps.is_available()\n",
    "\n",
    "hardware = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    hardware = \"cuda\"\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"MPS is available\")\n",
    "        hardware = \"mps\"\n",
    "        \n",
    "print(\"Hardware is set to: \", hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hieu/Projects/tiny-llm-agent/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 5183.91it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: OptimizedModule(\n",
      "  (_orig_mod): Qwen2ForCausalLM(\n",
      "    (model): Qwen2Model(\n",
      "      (embed_tokens): Embedding(151936, 896)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2Attention(\n",
      "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      (rotary_emb): Qwen2RotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# We use Qwen2.5-0.5B-Instruct as our local LLM model (~1GB)\n",
    "# Link to download the model: https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e\n",
    "model_path = \"../models/Qwen2.5-0.5B-Instruct\"  # local directory to save the model\n",
    "snapshot_download(repo_id=\"Qwen/Qwen2.5-0.5B-Instruct\", local_dir=model_path)\n",
    "\n",
    "if hardware == \"mps\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        device_map=\"auto\"\n",
    "        )\n",
    "else:\n",
    "    # Quantize the model\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config\n",
    "        )\n",
    "\n",
    "# Compile the model for faster execution\n",
    "model = torch.compile(model)\n",
    "print(\"model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-01T21:32:19-05:00', 'moddate': '2024-12-01T21:32:19-05:00', 'title': 'DeMo: Decoupled Momentum Optimization', 'subject': '', 'author': 'Bowen Peng, Jeffrey Quesnelle, Diederik P. Kingma', 'keywords': '', 'source': '../examples/example.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='arXiv:2411.19870v1  [cs.LG]  29 Nov 2024\\nDeMo: Decoupled Momentum Optimization\\nBowen Peng 1∗ Jeffrey Quesnelle 1† Diederik P . Kingma ‡\\n1Nous Research\\nAbstract\\nTraining large neural networks typically requires sharing gradients between ac-\\ncelerators through specialized high-speed interconnects . Drawing from the sig-\\nnal processing principles of frequency decomposition and e nergy compaction,\\nwe demonstrate that synchronizing full optimizer states an d model parameters\\nduring training is unnecessary. By decoupling momentum upd ates and allow-\\ning controlled divergence in optimizer states across accel erators, we achieve\\nimproved convergence compared to state-of-the-art optimi zers. W e introduce\\nDecoupled Momentum (DeMo), a fused optimizer and data parallel algorith m\\nthat reduces inter-accelerator communication requiremen ts by several orders of\\nmagnitude. This enables training of large neural networks e ven with limited\\nnetwork bandwidth and heterogeneous hardware. Our method i s topology-\\nagnostic and architecture-independent and supports scala ble clock-synchronous\\ndistributed training with negligible compute and memory ov erhead. Empiri-\\ncal results show that models trained with DeMo match or excee d the perfor-\\nmance of equivalent models trained with AdamW , while elimin ating the need\\nfor high-speed interconnects when pre-training large scal e foundation models.\\nAn open source reference PyT orch implementation is publish ed on GitHub at\\nhttps://github.com/bloc97/DeMo.\\n1 Introduction\\nLarge-scale neural networks, particularly language models, are characterized by high parameter\\ncounts. In fact, it is not uncommon to talk about models with t rillions of parameters. Training\\nthese models requires multiple accelerators (e.g. GPUs, TP Us) to achieve tractable training times.\\nCommon strategies for distributing training across accele rators include Distributed Data Parallelism\\n[5] and Fully Sharded Data Parallelism [13]. These techniqu es work by having accelerators split\\nthe weights and synchronize the gradients (sometimes multi ple times per step), with communication\\nvolumes proportional to the model size itself.\\nThis gradient synchronization between accelerators traditionally requires specialized high-speed in-\\nterconnects (e.g. Inﬁniband). Such interconnects represe nt expensive localized networking topolo-\\ngies, constraining all accelerators to be present in the sam e data center. However, if the volume of\\nsynchronized data could be substantially reduced, these ha rdware constraints could potentially be\\nrelaxed.\\nIn this paper, we demonstrate that gradients and optimizer states during the training of large neural\\nnetworks exhibit signiﬁcant redundancy and are highly comp ressible. Building on this insight, we\\ndevelop DeMo, an optimizer that takes advantage of this comp ressibility to reduce inter-accelerator\\ncommunication needs by several orders of magnitude. W e eval uated DeMo by training a standard\\n∗ X: @bloc97 Email: bloc@nousresearch.com\\n† X: @theemozilla Email: emozilla@nousresearch.com\\n‡ This work was done in the author’s personal capacity as an ind ependent researcher before joining Anthropic')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load a PDF document using PyMuPDF\n",
    "loader = PyPDFLoader(\"../examples/example.pdf\")\n",
    "docs = loader.load()  # a list of Document objects\n",
    "print(\"Number of pages:\", len(docs))\n",
    "docs[0]  # first page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arXiv:2411.19870v1  [cs.LG]  29 Nov 2024\\nDeMo: Decoupled Momentum Optimization\\nBowen Peng 1∗ Jeffrey Quesnelle 1† Diederik P . Kingma ‡\\n1Nous Research\\nAbstract\\nTraining large neural networks typically requires sharing gradients between ac-\\ncelerators through specialized high-speed interconnects . Drawing from the sig-\\nnal processing principles of frequency decomposition and e nergy compaction,\\nwe demonstrate that synchronizing full optimizer states an d model parameters\\nduring training is unnecessary. By decoupling momentum upd ates and allow-\\ning controlled divergence in optimizer states across accel erators, we achieve\\nimproved convergence compared to state-of-the-art optimi zers. W e introduce\\nDecoupled Momentum (DeMo), a fused optimizer and data parallel algorith m\\nthat reduces inter-accelerator communication requiremen ts by several orders of\\nmagnitude. This enables training of large neural networks e ven with limited',\n",
       " 'magnitude. This enables training of large neural networks e ven with limited\\nnetwork bandwidth and heterogeneous hardware. Our method i s topology-\\nagnostic and architecture-independent and supports scala ble clock-synchronous\\ndistributed training with negligible compute and memory ov erhead. Empiri-\\ncal results show that models trained with DeMo match or excee d the perfor-\\nmance of equivalent models trained with AdamW , while elimin ating the need\\nfor high-speed interconnects when pre-training large scal e foundation models.\\nAn open source reference PyT orch implementation is publish ed on GitHub at\\nhttps://github.com/bloc97/DeMo.\\n1 Introduction\\nLarge-scale neural networks, particularly language models, are characterized by high parameter\\ncounts. In fact, it is not uncommon to talk about models with t rillions of parameters. Training\\nthese models requires multiple accelerators (e.g. GPUs, TP Us) to achieve tractable training times.',\n",
       " 'these models requires multiple accelerators (e.g. GPUs, TP Us) to achieve tractable training times.\\nCommon strategies for distributing training across accele rators include Distributed Data Parallelism\\n[5] and Fully Sharded Data Parallelism [13]. These techniqu es work by having accelerators split\\nthe weights and synchronize the gradients (sometimes multi ple times per step), with communication\\nvolumes proportional to the model size itself.\\nThis gradient synchronization between accelerators traditionally requires specialized high-speed in-\\nterconnects (e.g. Inﬁniband). Such interconnects represe nt expensive localized networking topolo-\\ngies, constraining all accelerators to be present in the sam e data center. However, if the volume of\\nsynchronized data could be substantially reduced, these ha rdware constraints could potentially be\\nrelaxed.\\nIn this paper, we demonstrate that gradients and optimizer states during the training of large neural']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # maximum number of characters per chunk\n",
    "    chunk_overlap=100,  # number of characters to overlap between chunks\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for doc in docs:\n",
    "    chunk_list = text_splitter.split_text(doc.page_content)\n",
    "    for chunk in chunk_list:\n",
    "        chunks.append(chunk)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS \n",
    "\n",
    "def compute_content_hash(chunks: list, embedding_model_name: str) -> str:\n",
    "    \"\"\"Compute a hash based on document content and embedding model name.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks.\n",
    "        embedding_model_name (str): Model name.\n",
    "        \n",
    "    Returns:\n",
    "        Hash string.\n",
    "    \"\"\"\n",
    "    content = \"\".join(chunks) + embedding_model_name\n",
    "    return hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "\n",
    "def get_vector_store(chunks: list, embeddings: HuggingFaceEmbeddings, cache_dir: str) -> FAISS:\n",
    "    \"\"\"Retrieves a vector store from a list of text chunks using the given embeddings.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks.\n",
    "        embeddings (HuggingFaceEmbeddings): Embeddings object.\n",
    "        cache_dir (str): Directory to save the vector store.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS object.\n",
    "    \"\"\"\n",
    "    # Compute content hash\n",
    "    embedding_model_name = embeddings.model_name\n",
    "    current_hash = compute_content_hash(chunks, embedding_model_name)\n",
    "    \n",
    "    # Check if cached index exists and contains a valid hash\n",
    "    hash_file = os.path.join(cache_dir, \"content_hash.txt\")\n",
    "    if os.path.exists(cache_dir) and os.path.exists(hash_file):\n",
    "        with open(hash_file, \"r\") as f:\n",
    "            cached_hash = f.read().strip()\n",
    "            \n",
    "        if current_hash == cached_hash:\n",
    "            print(\"Loading cached FAISS index ...\")\n",
    "            return FAISS.load_local(\n",
    "                folder_path=cache_dir,\n",
    "                embeddings=embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"Cache invalidated due to changes in documents or embedding model.\")\n",
    "            import shutil\n",
    "            shutil.rmtree(cache_dir)\n",
    "            \n",
    "    # Create a new vector store\n",
    "    print(\"Creating a new FAISS index ...\")\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)    \n",
    "    \n",
    "    # Save the new index and hash\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    vector_store.save_local(cache_dir)\n",
    "    with open(hash_file, \"w\") as f:\n",
    "        f.write(current_hash)    \n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/fcbv08dn0xl7lvt7g_6jb7ch0000gn/T/ipykernel_47346/2954124615.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache invalidated due to changes in documents or embedding model.\n",
      "Creating a new FAISS index ...\n",
      "[Document(id='31441a37-7443-4720-9b50-ba59700a85c0', metadata={}, page_content='W e evaluated the signum variant of DeMo using OLMo [4], a highly reproducible large language\\nmodel pre-training framework. Adapting OLMo to use DeMo req uired only including the DeMo\\noptimizer class and disabling gradient synchronization in PyT orch Distributed Data Parallelism [5].\\nW e provide the modiﬁed OLMo code as well as the conﬁguration ﬁ les for all experiments in the\\nsupplementary material.\\nOur experiments used the Dolma v1.55 dataset for pre-training. As a baseline we used the publicly\\nreleased OLMo-1B 6, a standard decoder-only Transformer model consisting of 1 .18 billion param-\\neters using the AdamW optimizer ( β1 = 0.9, β2 = 0.95, weight decay = 0.1) as compared to\\nusing the DeMo optimizer ( β = 0.999). The learning rate and the AdamW hyperparameters were\\nuntouched and set with the suggested defaults.\\nDue to computational constraints, we trained models for 100billion total tokens rather than the'), Document(id='e24d438d-c05d-4786-a2a6-6b1cf07c87e5', metadata={}, page_content='LLM architecture (decoder-only Transformer [9]) using bot h the baseline optimizer (AdamW [6])\\nin traditional high-speed interconnects and DeMo in bandwi dth-constrained scenarios. Our results\\nshow that models trained with DeMo meet or exceed the perform ance of their conventional counter-\\nparts.\\nThe remainder of this paper is organized as follows. Section2 reviews the relevant background and\\nrelated work. Section 3 presents our methodology and theore tical foundations. Section 4 details our\\nexperimental setup and results. Finally, Section 5 conclud es with implications and future directions.\\n2 Background and Related W ork\\nV arious strategies have been developed to mitigate communication overhead in distributed training.\\nFor centralized and clock-synchronous training, the most e ffective techniques can be categorized\\ninto three main approaches:\\n• Quantization and sparsiﬁcation of gradients.\\n• Low-rank projection of gradients.\\n• Federated averaging (also known as Local-SGD).')]\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": hardware}\n",
    ")\n",
    "\n",
    "vector_store = get_vector_store(chunks, embeddings, \"../vector_store\")\n",
    "\n",
    "# Test a simple similarity search\n",
    "query = \"The goal of the transformer model is\"\n",
    "results = vector_store.search(query, k=2, search_type=\"similarity\")\n",
    "print(results)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  # search type: \"similarity\" or \"mmr\"\n",
    "    search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "The model 'OptimizedModule' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# RAG pipeline with a maximum length of max_new_tokens tokens\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "# Wrap the HuggingFace pipeline in a LangChain object\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the improved prompt template\n",
    "prompt_template = \"\"\"Answer based on context:\\n{context}\\nQuestion: {question}\\nAnswer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"contenxt\", \"question\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/fcbv08dn0xl7lvt7g_6jb7ch0000gn/T/ipykernel_47346/3082531246.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper describes an approach that uses deep learning models to improve natural language processing tasks such as machine translation by training them with large amounts of annotated data from different languages. It also discusses how these models can be integrated into existing systems like those found in Google Translate or Microsoft Translator, which are widely recognized as essential tools for human-machine interaction in international communication. Additionally, it highlights the challenges involved when developing efficient architectures capable of handling high volumes of text at scale while maintaining accuracy and performance. Furthermore, the authors emphasize the importance of reproducibility and comparison between their methods and other approaches in order to facilitate future improvements and comparisons. They provide detailed descriptions of each component of the architecture including encoder-decoder networks, attention mechanisms, cross-attention layers, pooling techniques, and fine-tuning strategies. Throughout the paper, they demonstrate the effectiveness of their proposed system through extensive experimental evaluation using various datasets and task-specific benchmarks. Finally, they discuss potential areas for further research within the field of neural machine translation and offer suggestions for improving efficiency and scalability. Overall, the paper presents a comprehensive overview of state-of-the-art approaches to addressing one of the most pressing problems facing modern AI applications, highlighting both strengths and weaknesses of current solutions.\n"
     ]
    }
   ],
   "source": [
    "question = \"Describe the main idea of the paper.\"\n",
    "response = qa_chain(question)\n",
    "answer = response[\"result\"].split(\"Answer:\")[-1].strip()\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
